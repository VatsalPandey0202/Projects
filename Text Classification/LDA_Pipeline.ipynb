{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "YKV9LvVGciQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code performs several tasks related to text processing, natural language processing (NLP), and topic modeling. It primarily uses libraries such as NumPy, pandas, scikit-learn, Gensim, NLTK, spaCy, and pyLDAvis. Let's break down the code step by step:\n",
        "\n",
        "1. **Importing Libraries:** This section imports various Python libraries that will be used throughout the script. These libraries include NumPy for numerical operations, pandas for data manipulation, and several NLP-related libraries such as Gensim, NLTK, spaCy, and scikit-learn for text processing and topic modeling. It also imports visualization libraries like matplotlib and pyLDAvis.\n",
        "\n",
        "2. **Fetching the 20 Newsgroups Dataset:**\n",
        "   - The code uses scikit-learn's `fetch_20newsgroups` function to load the 20 Newsgroups dataset, which is a collection of newsgroup documents grouped into categories.\n",
        "   - It loads both the training and testing subsets of the dataset.\n",
        "   - The `remove` parameter is used to remove specific parts of the text data, such as email headers, footers, and quotes, to prepare the data for further analysis.\n",
        "\n",
        "3. **Creating DataFrames:**\n",
        "   - After loading the dataset, the code creates two Pandas DataFrames, `news_train` and `news_test`, to organize the data. These DataFrames have two columns: 'news' (containing the text of the news articles) and 'class' (containing the class labels).\n",
        "\n",
        "4. **Merging DataFrames:**\n",
        "   - The code concatenates the training and testing DataFrames, `news_train` and `news_test`, into a single DataFrame called `df`. This concatenation combines both the training and testing data into one dataset.\n",
        "   - The resulting `df` DataFrame is reset so that it has a continuous index.\n",
        "\n",
        "5. **Displaying DataFrame Head:**\n",
        "   - The script concludes by displaying the first few rows of the merged DataFrame `df` using the `df.head()` method. This is done to provide a preview of the loaded and merged dataset.\n",
        "\n",
        "Overall, this code prepares the 20 Newsgroups dataset for further text analysis and topic modeling by loading, preprocessing, and merging the data into a convenient Pandas DataFrame structure. The dataset is now ready for additional text processing, topic modeling, and analysis tasks."
      ],
      "metadata": {
        "id": "Ag_TLMxjcSgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "t1D3NQ0Xb89_",
        "outputId": "75aebf0a-7608-4f53-f1d8-4e6d7ef91cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                news  class\n",
              "0  I was wondering if anyone out there could enli...      7\n",
              "1  A fair number of brave souls who upgraded thei...      4\n",
              "2  well folks, my mac plus finally gave up the gh...      4\n",
              "3  \\nDo you have Weitek's address/phone number?  ...      1\n",
              "4  From article <C5owCB.n3p@world.std.com>, by to...     14"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bedad2dd-0018-421d-aac4-b06d52b41a1a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was wondering if anyone out there could enli...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A fair number of brave souls who upgraded thei...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bedad2dd-0018-421d-aac4-b06d52b41a1a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bedad2dd-0018-421d-aac4-b06d52b41a1a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bedad2dd-0018-421d-aac4-b06d52b41a1a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24be3d23-7e17-463d-ba27-62804c7caacd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24be3d23-7e17-463d-ba27-62804c7caacd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24be3d23-7e17-463d-ba27-62804c7caacd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "#import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "# dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Import Dataset\n",
        "# loading train dataset\n",
        "news_group_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
        "news_group_data_train = news_group_train.data\n",
        "news_group_target_names_train = news_group_train.target_names\n",
        "news_group_target_train = news_group_train.target\n",
        "\n",
        "# Creating a dataframe from the loaded data\n",
        "news_train = pd.DataFrame({'news': news_group_data_train,\n",
        "                        'class': news_group_target_train})\n",
        "\n",
        "#Loading test data\n",
        "news_group_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
        "news_group_data_test = news_group_test.data\n",
        "news_group_target_names_test = news_group_test.target_names\n",
        "news_group_target_test = news_group_test.target\n",
        "\n",
        "# Creating a dataframe from the loaded data\n",
        "news_test = pd.DataFrame({'news': news_group_data_test,\n",
        "                        'class': news_group_target_test})\n",
        "#Merging both dataset\n",
        "frames = [news_train,news_test]\n",
        "df = pd.concat(frames).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares the 20 Newsgroups dataset for further text analysis and topic modeling by loading, preprocessing, and merging the data into a convenient Pandas DataFrame structure. It primarily uses libraries such as NumPy, pandas, scikit-learn, Gensim, NLTK, spaCy, and pyLDAvis. Let's break down the code step by step:\n",
        "\n",
        "1. **Importing Libraries:** This section imports various Python libraries that will be used throughout the script. These libraries include NumPy for numerical operations, pandas for data manipulation, and several NLP-related libraries such as Gensim, NLTK, spaCy, and scikit-learn for text processing and topic modeling. It also imports visualization libraries like matplotlib and pyLDAvis.\n",
        "\n",
        "2. **Fetching the 20 Newsgroups Dataset:**\n",
        "   - The code uses scikit-learn's `fetch_20newsgroups` function to load the 20 Newsgroups dataset, which is a collection of newsgroup documents grouped into categories.\n",
        "   - It loads both the training and testing subsets of the dataset.\n",
        "   - The `remove` parameter is used to remove specific parts of the text data, such as email headers, footers, and quotes, to prepare the data for further analysis.\n",
        "\n",
        "3. **Creating DataFrames:**\n",
        "   - After loading the dataset, the code creates two Pandas DataFrames, `news_train` and `news_test`, to organize the data. These DataFrames have two columns: 'news' (containing the text of the news articles) and 'class' (containing the class labels).\n",
        "\n",
        "4. **Merging DataFrames:**\n",
        "   - The code concatenates the training and testing DataFrames, `news_train` and `news_test`, into a single DataFrame called `df`. This concatenation combines both the training and testing data into one dataset.\n",
        "   - The resulting `df` DataFrame is reset so that it has a continuous index.\n",
        "\n",
        "5. **Displaying DataFrame Head:**\n",
        "   - The script concludes by displaying the first few rows of the merged DataFrame `df` using the `df.head()` method. This is done to provide a preview of the loaded and merged dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "gLBrw65Bik6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJbXJiGHb8-J",
        "outputId": "37b19bd4-a1d1-4f76-cb6a-a00b907147e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:19: DeprecationWarning: invalid escape sequence '\\S'\n",
            "<>:22: DeprecationWarning: invalid escape sequence '\\s'\n",
            "<>:19: DeprecationWarning: invalid escape sequence '\\S'\n",
            "<>:22: DeprecationWarning: invalid escape sequence '\\s'\n",
            "<ipython-input-5-c9898cc0a59a>:19: DeprecationWarning: invalid escape sequence '\\S'\n",
            "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
            "<ipython-input-5-c9898cc0a59a>:22: DeprecationWarning: invalid escape sequence '\\s'\n",
            "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class CleanTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        print(\"Inside cleaning init\")\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        print(\"Inside cleaning pipe fit\")\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "    def transform(self,X,y=None):\n",
        "        print(\"Inside cleaning pipe transform\")\n",
        "        # Convert to list\n",
        "        data = X.tolist()\n",
        "\n",
        "        # Remove Emails\n",
        "        data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "        # Remove new line characters\n",
        "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "        # Remove distracting single quotes\n",
        "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "\n",
        "        def sent_to_words(sentences):\n",
        "            for sentence in sentences:\n",
        "                yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "        data_words = list(sent_to_words(data))\n",
        "\n",
        "        def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "            \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "            texts_out = []\n",
        "            for sent in texts:\n",
        "                doc = nlp(\" \".join(sent))\n",
        "                texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "            return texts_out\n",
        "\n",
        "        # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "        # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
        "        data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "        return(data_lemmatized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a custom transformer class called `LDATransformer`, which is designed to fit and transform text data using Latent Dirichlet Allocation (LDA) for topic modeling. This class inherits from scikit-learn's `BaseEstimator` and `TransformerMixin` classes and follows the scikit-learn estimator and transformer API conventions. Overall, this custom transformer (`LDATransformer`) encapsulates the process of fitting an LDA model to text data and transforming the data into topic distributions. It can be used within a scikit-learn pipeline for text analysis and feature engineering.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "1. **Import Libraries:** The code imports necessary libraries, including scikit-learn's `CountVectorizer` for text vectorization and Gensim for topic modeling.\n",
        "\n",
        "2. **Initialize LDA Model:** An LDA model is initialized with specific parameters, such as the number of topics (20), maximum iterations, learning method, random state, batch size, and more. This LDA model will be used within the transformer.\n",
        "\n",
        "3. **Define `LDATransformer` Class:**\n",
        "   - The `LDATransformer` class is defined as a custom transformer.\n",
        "   - It has `fit` and `transform` methods, which are required by scikit-learn transformers.\n",
        "\n",
        "4. **`fit` Method:**\n",
        "   - The `fit` method takes the input text data `X` and performs the following steps:\n",
        "     - Creates a Gensim dictionary from the input text data.\n",
        "     - Computes the Term Document Frequency (TF-IDF) for the data.\n",
        "     - Builds an LDA model using Gensim, which learns the topics from the data.\n",
        "   - The number of topics, alpha, eta, and other LDA model parameters are specified within this method.\n",
        "   - The learned LDA model is stored as an attribute of the transformer.\n",
        "\n",
        "5. **`transform` Method:**\n",
        "   - The `transform` method takes the input text data `X` and performs the following steps:\n",
        "     - Computes the Term Document Frequency (TF-IDF) for the input data.\n",
        "     - Extracts the topics for each document in the input data using the pre-trained LDA model.\n",
        "     - Converts the extracted topic distributions into a Pandas DataFrame where each row represents a document and each column represents a topic. The values in the DataFrame indicate the weight of each topic for each document.\n",
        "     - Finally, the method returns the topic distributions as a NumPy array."
      ],
      "metadata": {
        "id": "987q8PD_jASH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS2FJCt1b8-M",
        "outputId": "6ca57060-8f41-4a5c-e50b-2a07e423d1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "count=0\n",
        "lda_model=LatentDirichletAllocation(n_components=20,               # Number of topics\n",
        "                                      max_iter=10,               # Max learning iterations\n",
        "                                      learning_method='online',\n",
        "                                      random_state=100,          # Random state\n",
        "                                      batch_size=128,            # n docs in each learning iter\n",
        "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
        "                                      n_jobs = -1,               # Use all available CPUs\n",
        "                                     )\n",
        "class LDATransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        print()\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "\n",
        "        # Create Dictionary\n",
        "        self.dictionary_LDA = corpora.Dictionary(X)\n",
        "\n",
        "        # Term Document Frequency\n",
        "        corpus = [self.dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in X]\n",
        "\n",
        "        # Build LDA model\n",
        "        self.num_topics = 20\n",
        "        self.lda_model = gensim.models.LdaModel(corpus, num_topics=self.num_topics, id2word=self.dictionary_LDA, passes=4, alpha=[0.01]*self.num_topics, \\\n",
        "                                           eta=[0.01]*len(self.dictionary_LDA.keys()))\n",
        "#         for i,topic in self.lda_model.show_topics(formatted=True, num_topics=self.num_topics, num_words=20):\n",
        "#             print(str(i)+\": \"+ topic)\n",
        "#             print()\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "        # Term Document Frequency\n",
        "        corpus = [self.dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in X]\n",
        "\n",
        "        topics = [self.lda_model[corpus[i]] for i in range(len(X))]\n",
        "        def topics_document_to_dataframe(topics_document, num_topics):\n",
        "            res = pd.DataFrame(columns=range(num_topics))\n",
        "            for topic_weight in topics_document:\n",
        "                res.loc[0, topic_weight[0]] = topic_weight[1]\n",
        "            return res\n",
        "        features=pd.concat([topics_document_to_dataframe(topics_document, num_topics=self.num_topics) for topics_document in topics]) \\\n",
        "            .reset_index(drop=True).fillna(0)\n",
        "        #print(features)\n",
        "        return features.to_numpy()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet uses scikit-learn's `train_test_split` function to split a dataset into training and testing sets for a machine learning task. Here's an explanation of what the code does:\n",
        "\n",
        "1. **Import the Necessary Library:**\n",
        "   - The code begins by importing the `train_test_split` function from scikit-learn's `model_selection` module.\n",
        "\n",
        "2. **Splitting the Dataset:**\n",
        "   - The `train_test_split` function is called with the following arguments:\n",
        "     - `df['news']`: This represents the feature data, which is the content of the news articles.\n",
        "     - `df['class']`: This represents the target variable or labels, which indicate the class or category of each news article.\n",
        "     - `test_size=0.33`: This specifies the proportion of the dataset that should be allocated to the testing set. In this case, 33% of the data is allocated to the testing set, while the remaining 67% is used for training.\n",
        "     - `random_state=0`: This sets the random seed for reproducibility. Setting a specific random seed ensures that the split is the same every time the code is run, making the results reproducible.\n",
        "     - `stratify=df['class']`: This parameter ensures that the class distribution in the original dataset is preserved in both the training and testing sets. It's particularly useful when dealing with imbalanced datasets.\n",
        "\n",
        "3. **Output Variables:**\n",
        "   - The `train_test_split` function returns four sets of data:\n",
        "     - `X_train`: This is the training data, which contains the news article content for the training set.\n",
        "     - `X_test`: This is the testing data, which contains the news article content for the testing set.\n",
        "     - `y_train`: These are the labels or target values for the training set, indicating the class of each news article in the training data.\n",
        "     - `y_test`: These are the labels or target values for the testing set, indicating the class of each news article in the testing data.\n",
        "\n",
        "The purpose of splitting the dataset into training and testing sets is to enable the training of a machine learning model on one subset and the evaluation of its performance on another. This helps assess the model's generalization ability and provides a way to estimate how well it will perform on unseen data."
      ],
      "metadata": {
        "id": "7dfJkWSqjRZC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1AwMWndb8-P",
        "outputId": "dfce74ad-c01b-45f8-f181-70e307ecc9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['news'],df['class'],test_size=0.33,random_state=0,stratify=df['class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code constructs a scikit-learn pipeline named `pipe`. A scikit-learn pipeline is a way to streamline and automate the process of transforming and modeling data, making it easier to work with complex workflows. The pipeline allows you to apply a sequence of transformations to your data and then fit a classifier on the transformed data with a single call. It simplifies the process of defining, fitting, and evaluating machine learning models, especially in situations where data preprocessing is involved. Here's an explanation of the code:\n",
        "\n",
        "1. **Import Necessary Libraries:**\n",
        "   - The code imports the required libraries:\n",
        "     - `Pipeline` from `sklearn.pipeline`: This class is used to create a machine learning pipeline that consists of multiple steps, such as data preprocessing, feature engineering, and modeling.\n",
        "     - `SVC` (Support Vector Classifier) from `sklearn.svm`: This is a support vector machine classifier that will be used as the final classification model in the pipeline.\n",
        "\n",
        "2. **Pipeline Initialization:**\n",
        "   - The `Pipeline` class is initialized with a list of named steps, where each step is a tuple consisting of a name (a string) and an estimator (an object that implements the scikit-learn estimator interface).\n",
        "\n",
        "3. **Pipeline Steps:**\n",
        "   - The pipeline consists of the following steps:\n",
        "     - `\"CleanTransformer\"`: This step is assumed to be a custom transformer that performs data cleaning and preprocessing on the input data. However, the code provided does not define the `CleanTransformer` class, so its behavior and implementation details are not visible in the provided code.\n",
        "     - `\"LDATransformer\"`: This step is also assumed to be a custom transformer that implements the `LDATransformer` class, as defined earlier in your code. It fits an LDA model to the text data and transforms it into topic distributions.\n",
        "     - `'svc'`: This step is the final classification model, which is an instance of the Support Vector Classifier (SVC) from scikit-learn. It is used for classifying the data after the text data has been preprocessed and transformed.\n",
        "\n",
        "Please note that for the code to work correctly, you should define the `CleanTransformer` class (if it's a custom transformer) and ensure that it is correctly imported before constructing the pipeline."
      ],
      "metadata": {
        "id": "izOOEqrejSOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDiTcw2b8-R",
        "outputId": "73afe3cc-9ff8-4c7e-fd17-8968bf0769a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside cleaning init\n",
            "Inside lda pipe init\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "pipe = Pipeline([(\"CleanTransformer\", CleanTransformer()),(\"LDATransformer\", LDATransformer()),('svc', SVC())])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code `pipe.fit(X_train, y_train)` fits the scikit-learn pipeline (`pipe`) to the training data. Here's what this code does:\n",
        "\n",
        "1. `X_train`: This is the training data, typically a feature matrix, containing the independent variables (in this case, the preprocessed text data) used to train the model.\n",
        "\n",
        "2. `y_train`: These are the labels or target values corresponding to the training data. They indicate the correct categories or classes for each data point in `X_train`.\n",
        "\n",
        "When you call `pipe.fit(X_train, y_train)`, the following steps occur:\n",
        "\n",
        "- The data in `X_train` is passed through the pipeline, where each step (transformer or estimator) is applied sequentially. This includes any data preprocessing, feature engineering, and modeling defined in the pipeline.\n",
        "\n",
        "- If you have a custom data cleaning or preprocessing step (e.g., the \"CleanTransformer\" step mentioned earlier in your pipeline), it will be applied first.\n",
        "\n",
        "- Next, the \"LDATransformer\" step is applied, where the LDA model is fitted to the text data in `X_train`, and the text is transformed into topic distributions.\n",
        "\n",
        "- Finally, the Support Vector Classifier (SVC) model, defined as 'svc' in the pipeline, is trained on the transformed data.\n",
        "\n",
        "- The labels in `y_train` are used to train the SVC model to learn the relationship between the text data and the corresponding class labels.\n",
        "\n",
        "After calling `pipe.fit(X_train, y_train)`, the pipeline is now fully trained and ready to make predictions on new data or be evaluated on the test dataset. It encapsulates all the necessary preprocessing and modeling steps, making it convenient to work with machine learning workflows."
      ],
      "metadata": {
        "id": "34UCURhtjTwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "tXCFx6Tbb8-T",
        "outputId": "5f5efdd2-eb4c-46f5-d36c-51fe9f55e25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside cleaning pipe fit\n",
            "Inside cleaning pipe transform\n",
            "Inside lda pipe fit\n",
            "Inside lda pipe transform\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('CleanTransformer', CleanTransformer()),\n",
              "                ('LDATransformer', LDATransformer()), ('svc', SVC())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;CleanTransformer&#x27;, CleanTransformer()),\n",
              "                (&#x27;LDATransformer&#x27;, LDATransformer()), (&#x27;svc&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;CleanTransformer&#x27;, CleanTransformer()),\n",
              "                (&#x27;LDATransformer&#x27;, LDATransformer()), (&#x27;svc&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CleanTransformer</label><div class=\"sk-toggleable__content\"><pre>CleanTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LDATransformer</label><div class=\"sk-toggleable__content\"><pre>LDATransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "pipe.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code `pipe.score(X_test, y_test)` is used to calculate the accuracy score of the machine learning pipeline (`pipe`) on the test dataset (`X_test` and `y_test`). Specifically, it computes how well the pipeline's classifier (in this case, the Support Vector Classifier, SVC) performs in making predictions on the test data and comparing those predictions to the true labels.\n",
        "\n",
        "Here's what the code does:\n",
        "\n",
        "- `X_test`: This is the test data, typically a feature matrix, containing the independent variables (in this case, the preprocessed text data) used to evaluate the model's performance.\n",
        "\n",
        "- `y_test`: These are the true labels or target values corresponding to the test data. They indicate the correct categories or classes for each data point in `X_test`.\n",
        "\n",
        "When you call `pipe.score(X_test, y_test)`, the following steps occur:\n",
        "\n",
        "1. The pipeline (`pipe`) takes the test data (`X_test`) and performs the same sequence of transformations and predictions as during training.\n",
        "\n",
        "2. The text data in `X_test` is preprocessed, including any data cleaning, topic modeling, or other transformations defined in the pipeline.\n",
        "\n",
        "3. The trained Support Vector Classifier (SVC) model in the pipeline makes predictions on the preprocessed test data.\n",
        "\n",
        "4. These predicted labels are then compared to the true labels (`y_test`) to compute the accuracy score.\n",
        "\n",
        "5. The accuracy score represents the fraction of correctly classified data points in the test dataset. It's a common metric for classification tasks and provides an estimate of the model's performance.\n",
        "\n",
        "The `pipe.score(X_test, y_test)` call returns a single accuracy score, indicating how accurately the pipeline's classifier predicts the class labels on the test data. The higher the accuracy score, the better the model's performance in classifying the test data."
      ],
      "metadata": {
        "id": "7EfMh7UpjVBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zLSkS3gb8-V",
        "outputId": "e901d922-f140-4a9a-bff8-20acee5a06a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside cleaning pipe transform\n",
            "Inside lda pipe transform\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4289389067524116"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "pipe.score(X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}